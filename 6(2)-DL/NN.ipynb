{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Instructions:*\n",
    "- **과제 명세서를 읽어주시고 코드 작성을 해주시길 바랍니다**</span> \n",
    "- **명시된 step을 따라가며 전체적인 학습 방법을 숙지합니다**</span>\n",
    "- (**첫 번째 cell 결과로 나온 시간을 기준으로 채점을 하겠습니다**</span>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2025-01-27 23:33:44.083950\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Mutilayer Perceptron(```class MutiLayerPerceptron```)으로 간단한 Binary classification task를 진행해볼 것입니다. \n",
    "\n",
    "> 1. **Dataset**\n",
    ">> $\\texttt{moon}$ dataset\n",
    "> 2. **Network architecture**\n",
    "\n",
    " > $H_1 = X \\cdot W_1 + b_1$   \n",
    " > $z_1 = ReLU(H_1)$ where $ReLU$($=\\max(0,x)$) is a rectified linear unit and $z_1$ is an output of the first hidden layer.  \n",
    " > $H_2 = z_1 \\cdot W_2 + b_2$   \n",
    " > $z_2 = LeakyReLU(H_2)$ where $LeakyReLU$($=\\max(0.01x,x)$) and $z_2$ is an output of the second hidden layer. \n",
    " > $H_3 = z_2 \\cdot W_3 + b_3$   \n",
    " > $z_3 = tanh(H_3 + H_1)$ where $\\tanh$ is a tanh function and $z_3$ is an output of the third hidden layer.  \n",
    " > $H_4 = z_3 \\cdot W_4 + b_4$   \n",
    " > $\\hat y = \\sigma(H_4)$ where $\\sigma$ is a sigmoid function unit and $\\hat y$ is an output of the network.\n",
    " \n",
    " > **$W$** and **$b$**는 각각 weights와 bias.    \n",
    " > **weight 초기화**: Standard normal ($\\texttt{np.random.randn}$. 사용)  \n",
    " > **bias 초기화(intercept)**: 0     \n",
    " > **Input size**: 2  \n",
    " > **The first hidden layer size**: 10  \n",
    " > **The second hidden layer size**: 10  \n",
    " > **Output size**: 1   \n",
    " > **Regularization parameter $\\lambda$**: 0.001  \n",
    " > **Loss function**: Binary cross entropy loss (or equivently log loss).  \n",
    " > **Total loss** : \n",
    " > $L_{total} = \\sum_{i=1}^N{ (-y^{(i)}\\log \\hat{y}^{(i)} -(1-y^{(i)})\\log(1-\\hat{y}^{(i)})) } +  \\lambda \\|W\\|^2 $   \n",
    " > **Optimization**: Gradient descent  \n",
    " > **Learning rate** = 0.0001  \n",
    " > **Number of epochs** = 50000  \n",
    " > $y$는 정답, $\\hat{y}$는 예측값이고 0부터 1사이에 존재한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "from mlp import MultiLayerPerceptron\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Load data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10dad38f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"STEP 1: Load data\")\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = sklearn.datasets.make_moons(300, noise = 0.25)\n",
    "\n",
    "# Visualize data\n",
    "plt.scatter(X_train[:,0], X_train[:,1], s = 40, c=y_train, cmap=plt.cm.RdYlGn)\n",
    "#왜 시각화가 안 되는 지 모르겠습니다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Train the model\n",
      "Loss (epoch 1000): 68.78064419717121\n",
      "Loss (epoch 2000): 47.96289120843212\n",
      "Loss (epoch 3000): 41.14618674256958\n",
      "Loss (epoch 4000): 38.63042318513523\n",
      "Loss (epoch 5000): 37.5785078664301\n",
      "Loss (epoch 6000): 36.3615112620398\n",
      "Loss (epoch 7000): 35.774411412034546\n",
      "Loss (epoch 8000): 35.24363444492326\n",
      "Loss (epoch 9000): 34.76226680080903\n",
      "Loss (epoch 10000): 34.361011761217924\n",
      "Loss (epoch 11000): 34.06551901493429\n",
      "Loss (epoch 12000): 33.68029004055791\n",
      "Loss (epoch 13000): 33.33135627410304\n",
      "Loss (epoch 14000): 32.98191317659962\n",
      "Loss (epoch 15000): 32.581038916560914\n",
      "Loss (epoch 16000): 32.259900495188226\n",
      "Loss (epoch 17000): 31.976641841745163\n",
      "Loss (epoch 18000): 31.746486769802377\n",
      "Loss (epoch 19000): 31.48447512979734\n",
      "Loss (epoch 20000): 31.190244040671036\n",
      "Loss (epoch 21000): 30.929741593278514\n",
      "Loss (epoch 22000): 30.69753888430843\n",
      "Loss (epoch 23000): 30.47760447485932\n",
      "Loss (epoch 24000): 30.27809930017289\n",
      "Loss (epoch 25000): 30.09752514445035\n",
      "Loss (epoch 26000): 29.927479799952152\n",
      "Loss (epoch 27000): 29.77910410072764\n",
      "Loss (epoch 28000): 29.6353136555402\n",
      "Loss (epoch 29000): 29.49602254205599\n",
      "Loss (epoch 30000): 29.373003301588057\n",
      "Loss (epoch 31000): 29.262719829674207\n",
      "Loss (epoch 32000): 29.164430494438797\n",
      "Loss (epoch 33000): 29.07472522716479\n",
      "Loss (epoch 34000): 28.99183921520721\n",
      "Loss (epoch 35000): 28.917665370329114\n",
      "Loss (epoch 36000): 28.85279896770074\n",
      "Loss (epoch 37000): 28.792139231958846\n",
      "Loss (epoch 38000): 28.73360569788118\n",
      "Loss (epoch 39000): 28.68585046059207\n",
      "Loss (epoch 40000): 28.627489637625015\n",
      "Loss (epoch 41000): 28.570048929877895\n",
      "Loss (epoch 42000): 28.517808046086174\n",
      "Loss (epoch 43000): 28.464182626313864\n",
      "Loss (epoch 44000): 28.418979867409142\n",
      "Loss (epoch 45000): 28.379472153419364\n",
      "Loss (epoch 46000): 28.33951520127133\n",
      "Loss (epoch 47000): 28.309034216258915\n",
      "Loss (epoch 48000): 28.27673576221195\n",
      "Loss (epoch 49000): 28.245235058924187\n",
      "Loss (epoch 50000): 28.22087894551914\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 2: Train the model\")\n",
    "# random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "nn_input_dim = 2\n",
    "nn_output_dim = 1\n",
    "nn_hdim1 = 10\n",
    "nn_hdim2 = 10\n",
    "nn_hdim3 = 10\n",
    "lr = 0.0001 \n",
    "L2_norm = 0.001\n",
    "epoch = 50000\n",
    "\n",
    "model = MultiLayerPerceptron(nn_input_dim, nn_hdim1, nn_hdim2, nn_hdim3, nn_output_dim, init=\"random\")\n",
    "stats = model.train(X_train, y_train, learning_rate=lr, L2_norm=L2_norm, epoch=epoch, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Plot decision boundary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Decision Boundary: Hidden layer dimension (10, 10)')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"STEP 3: Plot decision boundary\")\n",
    "# Plot the decision boundary\n",
    "utils.plot_decision_boundary(lambda x: model.predict(x), X_train, y_train)\n",
    "plt.title(f\"Decision Boundary: Hidden layer dimension {nn_hdim1, nn_hdim2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(stats['loss_history'])) * 1000, stats['loss_history'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss over epoch')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(len(stats['train_acc_history'])) * 1000, stats['train_acc_history'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.title('Training accuracy over epoch')\n",
    "plt.gcf().set_size_inches(20, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ybenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
