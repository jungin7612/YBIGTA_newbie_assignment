import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from typing import Any
from resnet import ResNet, BasicBlock
from config import *

NUM_CLASSES = 10

# ðŸ”¹ **ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¦ê°•(Augmentation)**
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),  # 32x32 í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ ëžœë¤í•˜ê²Œ ìžë¦„
    transforms.RandomHorizontalFlip(),  # ëžœë¤ ì¢Œìš° ë°˜ì „
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

# ðŸ”¹ **ë©”ì¸ ì‹¤í–‰ë¶€ë¥¼ `if __name__ == "__main__":` ì•„ëž˜ë¡œ ì´ë™**
if __name__ == "__main__":
    device = torch.device(
        "mps" if torch.backends.mps.is_available() else "cpu")
    print(f"Using device: {device}")

    # ðŸ”¹ **CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ**
    train_dataset = datasets.CIFAR10(
        root="./data", train=True, transform=transform_train, download=True)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,
                              shuffle=True, num_workers=0)  # Macì—ì„œëŠ” `num_workers=0` ì„¤ì •

    test_dataset = datasets.CIFAR10(
        root="./data", train=False, transform=transform_test, download=True)
    test_loader = DataLoader(
        test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    # ðŸ”¹ **ResNet-18 ëª¨ë¸ ì„ ì–¸**
    model = ResNet(BasicBlock, [2, 2, 2, 2],
                   num_classes=NUM_CLASSES).to(device)

    # ðŸ”¹ **ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •**
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(
        model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)
    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)

    def train(model: nn.Module, loader: DataLoader, criterion: nn.Module, optimizer: optim.Optimizer, device: torch.device) -> None:
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        print("ðŸ”¹ Training started...")  # ë””ë²„ê¹…ìš© ë©”ì‹œì§€

        for batch_idx, (inputs, targets) in enumerate(loader):
            # í˜„ìž¬ ë°°ì¹˜ ë²ˆí˜¸ ì¶œë ¥
            print(f"ðŸ”¹ Processing batch {batch_idx + 1}/{len(loader)}")
            inputs, targets = inputs.to(device), targets.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        accuracy = 100. * correct / total
        print(
            f"Train Loss: {total_loss / len(loader):.4f}, Accuracy: {accuracy:.2f}%")
        print("ðŸ”¹ Training finished!")  # ë””ë²„ê¹…ìš© ë©”ì‹œì§€

    # ðŸ”¹ **í‰ê°€ í•¨ìˆ˜**

    def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module, device: torch.device) -> float:
        model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, targets in loader:
                inputs, targets = inputs.to(device), targets.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, targets)

                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

        accuracy = 100. * correct / total
        print(
            f"Test Loss: {total_loss / len(loader):.4f}, Accuracy: {accuracy:.2f}%")
        return accuracy  # Early Stoppingì„ ìœ„í•œ ë°˜í™˜ê°’

    # ðŸ”¹ **í•™ìŠµ ë° í‰ê°€ ë£¨í”„**
    best_accuracy: float = 0
    patience = 5
    counter = 0

    for epoch in range(EPOCHS):
        print(f"Epoch {epoch + 1}/{EPOCHS}")
        train(model, train_loader, criterion, optimizer, device)
        accuracy = evaluate(model, test_loader, criterion, device)

        scheduler.step()

        # Early Stopping
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            counter = 0
            torch.save(model.state_dict(), "best_model.pth")
            print("Model saved.")
        else:
            counter += 1
            if counter >= patience:
                print("Early stopping triggered.")
                break

    print(f"Best Test Accuracy: {best_accuracy:.2f}%")
